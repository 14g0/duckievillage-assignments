{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adolescent-thickness",
   "metadata": {
    "id": "adolescent-thickness"
   },
   "source": [
    "# Identifica√ß√£o\n",
    "\n",
    "**Nome:** preencher nome aqui\n",
    "\n",
    "**NUSP:** preencher n√∫mero USP aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-contamination",
   "metadata": {
    "id": "voluntary-contamination"
   },
   "source": [
    "# MAC0318 Introdu√ß√£o √† Programa√ß√£o de Rob√¥s M√≥veis\n",
    "\n",
    "## Aprendizado por refor√ßo profundo\n",
    "\n",
    "Nesse notebook iremos implementar o algoritmo de controle DQN, um algoritmo de aprendizado por refor√ßo profundo, que combina redes neurais e programa√ß√£o din√¢mica aproximada. A proposta geral do DQN √© combinar a regra de atualiza√ß√£o do Q-Learning com aproximadores de fun√ß√£o n√£o-lineares para se obter boa generaliza√ß√£o sobre o conjunto de poss√≠veis observa√ß√µes.\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/control.png?raw=true\" alt=\"Agent-Env Loop\" style=\"width: 300px;\"/>\n",
    "\n",
    "Neste notebook o nosso objetivo ser√° resolver o ambiente `CartPole-v1`. \n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Entender o papel da otimalidade de Bellman para algoritmos de controle\n",
    "- Desenvolver intui√ß√£o sobre o problema de explora√ß√£o em RL\n",
    "- Ter um primeiro contato com t√©cnicas de treinamento de algoritmos de deep RL\n",
    "- Familiarizar-se com a biblioteca de redes neurais dm-sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-title",
   "metadata": {
    "id": "illegal-title"
   },
   "source": [
    "### üíª Configura√ß√µes\n",
    "\n",
    "√â necess√°rio rodar a c√©lula abaixo apenas uma vez para instalar as depend√™ncias do notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a399b488-1f22-4dd8-92b6-2d4f385e74e6",
   "metadata": {
    "id": "a399b488-1f22-4dd8-92b6-2d4f385e74e6"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# prepara a m√°quina local do google colab para receber os arquivos base\n",
    "# e baixa os scripts auxiliares para a montagem do notebook\n",
    "\n",
    "[[ ! -e /opt/google ]] && exit # Caso esta c√©lula n√£o esteja executando no Colab, interrompe sua execu√ß√£o aqui\n",
    "[[ ! -e ./ckpt ]] && exit # Caso j√° exista o checkpoint, interrompe sua execu√ß√£o aqui\n",
    "\n",
    "git clone https://github.com/thiagopbueno/curso-verao-rl-ime-2021.git --quiet\n",
    "mv ./curso-verao-rl-ime-2021/notebooks/aula3/* ./\n",
    "\n",
    "rm -rf ./curso-verao-rl-ime-2021\n",
    "rm -rf ./img\n",
    "rm -rf ./ckpt/ddqn-pongnoframeskip-v4/\n",
    "rm -rf ./Aula\\ 3\\ -\\ Parte\\ Pr√°tica\\ -\\ Problema\\ de\\ Controle\\ -\\ DQN.ipynb\n",
    "rm -rf ./Solu√ß√£o\\ -\\ Aula\\ 3\\ -\\ Parte\\ Pr√°tica\\ -\\ Problema\\ de\\ Controle\\ -\\ DQN-Copy1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21817d03-c332-4bca-9393-2e6f1a0a5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar pacotes\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-liberty",
   "metadata": {
    "id": "distinct-liberty"
   },
   "source": [
    "> N√£o se esque√ßa de executar os imports abaixo antes de prosseguir com o notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleased-vacuum",
   "metadata": {
    "id": "pleased-vacuum"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack, Monitor, TimeLimit\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from utils import logging\n",
    "from utils.nn import initializers\n",
    "from utils import replay\n",
    "from utils import schedule\n",
    "from utils import tf_utils\n",
    "\n",
    "tf_utils.set_tf_allow_growth() # necess√°rio apenas se voc√™ disp√µe de GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-disco",
   "metadata": {
    "id": "worldwide-disco"
   },
   "source": [
    "## Ambiente - CartPole-v1\n",
    "\n",
    "Como veremos neste exerc√≠cio-programa, √© sempre uma boa ideia em aprendizado por refor√ßo iniciar o estudo de um algoritmo por um problema simples e pequeno para o qual voc√™ poder√° resolver em poucos minutos. Para isso, o ambiente do `CartPole-v1` √© usualmente um dos primeiros problemas que um agente baseado em aprendizado por refor√ßo deve ser capaz de resolver antes de tentar atacar problemas mais complexos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bearing-assistant",
   "metadata": {
    "id": "bearing-assistant"
   },
   "outputs": [],
   "source": [
    "def make_envs(env_id):\n",
    "    env = gym.make(env_id)\n",
    "    eval_env = gym.vector.make(env_id, num_envs=20, asynchronous=True)\n",
    "    test_env = gym.make(env_id)\n",
    "    return env, eval_env, test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "military-physiology",
   "metadata": {
    "id": "military-physiology"
   },
   "outputs": [],
   "source": [
    "env, eval_env, test_env = make_envs(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-serve",
   "metadata": {
    "id": "superior-serve"
   },
   "source": [
    "> Dica: se voc√™ n√£o estiver familiarizado com esse ambiente ou precisar refrescar a mem√≥ria, lembre-se de consultar a documenta√ß√£o dispon√≠vel no site do OpenAI Gym [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control) e tamb√©m procure entender principalmente os detalhes sobre o espa√ßo de estados e a√ß√µes do ambiente usando os m√©todos `env.observation_space` e `env.action_space`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-court",
   "metadata": {
    "id": "parental-court"
   },
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "Como visto em aula o algoritmo `DQN` procura aproximar a fun√ß√£o $Q(s, a)$ utilizando redes neurais treinadas por meio da otimiza√ß√£o de uma fun√ß√£o objetivo baseada na regra de atualiza√ß√£o do Q-Learning. O algoritmo abaixo descreve de maneira geral o treinamento de um agente `DQN`.\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/dqn-algo.png?raw=true\"/>\n",
    "\n",
    "Nessa se√ß√£o desenvolveremos os componentes desse algoritmo passo a passo:\n",
    "\n",
    "1. **Redes neurais (networks)**: inicialmente construiremos usando a biblioteca `dm-sonnet` a rede neural para a fun√ß√£o $Q(s, a)$;\n",
    "2. **Fun√ß√£o objetivo (loss)**: uma vez definida a classe da fun√ß√£o $Q(s, a)$, implementaremos a fun√ß√£o objetivo utilizada no problema de \"regress√£o\" que o Q-Learning tenta resolver;\n",
    "3. **Atualiza√ß√£o (update)**: em seguida instanciaremos um otimizador baseado em gradientes que ser√° respons√°vel por minimizar a fun√ß√£o objetivo previamente definida; e\n",
    "4. **Pol√≠tica $\\epsilon$-greedy**: por fim definiremos a pol√≠tica estoc√°stica para explora√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-organic",
   "metadata": {
    "id": "saved-organic"
   },
   "source": [
    "### Redes Neurais (networks)\n",
    "\n",
    "Para representar fun√ß√µes $Q(s,a)$ utilizando redes neurais, temos em geral 2 op√ß√µes de implementa√ß√£o:\n",
    "1. Definir uma rede com entrada $(s, a)$ e sa√≠da um √∫nico n√∫mero real: $Q_\\phi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$; ou\n",
    "2. Definir como entrada apenas o estado $s$ e sa√≠da um vetor de tamanho $|\\mathcal{A}|$: $Q_\\phi : \\mathcal{S} \\rightarrow \\mathbb{R}^{|\\mathcal{A}|}$ .\n",
    "\n",
    "Em geral, para algoritmos baseados no `DQN` √© costume utilizar a 2a op√ß√£o.\n",
    "\n",
    "> **Observa√ß√£o**: note que $\\phi \\in \\mathbb{R}^d$ com $d \\ll |S|$, onde $\\phi$ denota o conjunto de par√¢metros (i.e., *kernels* e *biases*) da rede neural. Dessa forma, a rede deve extrair apenas informa√ß√µes essenciais sobre o estado para a predi√ß√£o do retorno esperado (como vimos na aula de predi√ß√£o).\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/conv-net.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stylish-business",
   "metadata": {
    "id": "stylish-business"
   },
   "outputs": [],
   "source": [
    "class QNetwork(snt.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, name=\"QNetwork\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # features\n",
    "        self._torso = snt.nets.MLP(\n",
    "            [8, 8],\n",
    "            activation=tf.nn.relu,\n",
    "            activate_final=True,\n",
    "            w_init=initializers.he_initializer(),\n",
    "            name=\"MLP\"\n",
    "        ) \n",
    "\n",
    "        # predictor\n",
    "        self._q_values = snt.Linear(action_space.n, name=\"QValues\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Calcula os Q-values de todas as a√ß√µes para uma dada `obs`.\"\"\"\n",
    "        h = self._torso(obs)\n",
    "        return self._q_values(h)\n",
    "\n",
    "    @tf.function\n",
    "    def action_values(self, obs, actions):\n",
    "        \"\"\"Calcula os Q-values de uma √∫nica `action` espec√≠fica para uma dada `obs`.\"\"\"\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        indices = tf.stack([tf.range(batch_size, dtype=actions.dtype), actions], axis=1)\n",
    "        q_values = tf.gather_nd(self(obs), indices)\n",
    "        return q_values\n",
    "\n",
    "    @tf.function\n",
    "    def hard_update(self, other):\n",
    "        \"\"\"Copia os par√¢metros da rede `other` para a rede do objeto.\"\"\"\n",
    "        for self_var, other_var in zip(self.trainable_variables, other.trainable_variables):\n",
    "            self_var.assign(other_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-flood",
   "metadata": {
    "id": "metropolitan-flood"
   },
   "source": [
    "### Fun√ß√£o objetivo (loss)\n",
    "\n",
    "Lembre-se que o `DQN` se utiliza da regra de atualiza√ß√£o baseado em programa√ß√£o din√¢mica aproximada do Q-Learning:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} [(Q_{\\phi}(s, a) - (r + \\gamma \\max_{a'} Q_{\\bar{\\phi}}(s', a')))^2]\n",
    "$$\n",
    "\n",
    "> **Observa√ß√£o**: lembre que para compor o \"alvo\" da regress√£o usamos a rede target $Q_{\\bar{\\phi}}$. Conforme vimos na aula te√≥rica, o uso de *target networks* √© fundamental para melhorar a estabilidade do treinamento. Caso contr√°rio, toda atualiza√ß√£o na dire√ß√£o de $\\nabla_{\\phi} \\mathcal{L}(\\phi)$ acabaria por alterar tamb√©m o valor do \"alvo\" da regress√£o, tornando o problema de otimiza√ß√£o muito mais complicado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "civil-durham",
   "metadata": {
    "id": "civil-durham"
   },
   "outputs": [],
   "source": [
    "def make_q_learning_loss(q_net, target_q_net, gamma=0.99):\n",
    "    \"\"\"Recebe a rede online `q_net` e a rede `target_q_net` e devolve o loss function do Q-Learning.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def _loss(batch):\n",
    "        \"\"\"Recebe um batch de experi√™ncias e devolve o valor da fun√ß√£o objetivo para esse batch.\"\"\"\n",
    "        obs = batch[\"obs\"]\n",
    "        actions = batch[\"action\"]\n",
    "        rewards = batch[\"reward\"]\n",
    "        next_obs = batch[\"next_obs\"]\n",
    "        terminals = tf.cast(batch[\"terminal\"], tf.float32)\n",
    "        \n",
    "        # predictions\n",
    "        q_values = q_net.action_values(obs, actions)\n",
    "\n",
    "        # targets\n",
    "        next_q_values = tf.reduce_max(target_q_net(next_obs), axis=-1)\n",
    "        q_targets = tf.stop_gradient(rewards + (1 - terminals) * gamma * next_q_values)\n",
    "\n",
    "        # loss = tf.reduce_mean((q_values - q_targets) ** 2)\n",
    "        loss = tf.losses.huber(q_values, q_targets)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-regression",
   "metadata": {
    "id": "composed-regression"
   },
   "source": [
    "### Atualiza√ß√£o (updates)\n",
    "\n",
    "Uma vez com *loss function* definida, basta instanciar um otimizador escolhendo uma taxa de aprendizado (i.e., `learning_rate`) rodando a c√©lula abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blank-watson",
   "metadata": {
    "id": "blank-watson"
   },
   "outputs": [],
   "source": [
    "def make_update_fn(loss_fn, trainable_variables, learning_rate=1e-3):\n",
    "    optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def _update_fn(batch):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            loss = loss_fn(batch)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "        grads_and_vars = {var.name: (grad, var) for grad, var in zip(grads, trainable_variables)}\n",
    "\n",
    "        return loss, grads_and_vars\n",
    "\n",
    "    return _update_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-decrease",
   "metadata": {
    "id": "remarkable-decrease"
   },
   "source": [
    "### Pol√≠tica $\\epsilon$-*greedy*\n",
    "\n",
    "O √∫ltimo componente do algoritmo `DQN` √© sua pol√≠tica utilizada para explorar. No notebook de hoje, implementaremos a pol√≠tica explorat√≥ria mais simples.\n",
    "\n",
    "Como vimos na aula te√≥rica, a pol√≠tica $\\epsilon$-*greedy* escolhe uma a√ß√£o aleat√≥ria com probabilidade $\\epsilon$ e escolhe a a√ß√£o gulosa com probabilidade $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "endless-tyler",
   "metadata": {
    "id": "endless-tyler"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, q_net, start_val=1.0, end_val=0.01, start_step=1_000, end_step=10_000):\n",
    "        self.q_net = q_net\n",
    "\n",
    "        self._schedule = schedule.PiecewiseLinearSchedule((start_step, start_val), (end_step, end_val))\n",
    "\n",
    "        self._step = tf.Variable(0., dtype=tf.float32, name=\"step\")\n",
    "        self._epsilon = tf.Variable(start_val, dtype=tf.float32, name=\"epsilon\")\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Retorna a√ß√£o aleat√≥ria com probabilidade epsilon, c.c., retorna a√ß√£o gulosa.\"\"\"\n",
    "        self._epsilon.assign(self._schedule(self._step))\n",
    "        self._step.assign_add(1)\n",
    "\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        action_dim = self.q_net.action_space.n\n",
    "\n",
    "        # os alunos devem implementar as duas linhas abaixo\n",
    "        random_actions = tf.random.uniform(shape=(batch_size,), minval=0, maxval=action_dim, dtype=tf.int32)\n",
    "        greedy_actions = tf.argmax(self.q_net(obs), axis=-1, output_type=tf.int32)\n",
    "\n",
    "        return tf.where(\n",
    "            self._epsilon > tf.random.uniform(shape=(batch_size,)),\n",
    "            random_actions,\n",
    "            greedy_actions            \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-emerald",
   "metadata": {
    "id": "valuable-emerald"
   },
   "source": [
    "### Agente DQN\n",
    "\n",
    "Com todos os componentes definidos, estamos finalmente preparados para instanciar um agente `DQN` para o ambiente `CartPole-v1`. Execute a c√©lula abaixo para criar a classe `DQN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cleared-accident",
   "metadata": {
    "id": "cleared-accident"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        gamma=0.99,\n",
    "        target_update_freq=1000,\n",
    "        learning_rate=1e-3, \n",
    "        policy_start_val=1.0, \n",
    "        policy_end_val=0.01, \n",
    "        policy_start_step=1_000, \n",
    "        policy_end_step=10_000,\n",
    "        checkpoint_dir=\"ckpt\"\n",
    "    ):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.q_net = QNetwork(self.observation_space, self.action_space, name=\"QNet\")\n",
    "        self.target_q_net = QNetwork(self.observation_space, self.action_space, name=\"TargetQNet\")\n",
    "\n",
    "        self.policy = EpsilonGreedyPolicy(self.q_net, start_val=policy_start_val, end_val=policy_end_val, start_step=policy_start_step, end_step=policy_end_step)\n",
    "        \n",
    "        self._ckpt_dir = checkpoint_dir\n",
    "        self._ckpt = tf.train.Checkpoint(q_net=self.q_net)\n",
    "        self._ckpt_manager = tf.train.CheckpointManager(self._ckpt, directory=self._ckpt_dir, max_to_keep=1)\n",
    "\n",
    "        self._step = tf.Variable(0, dtype=tf.int32, name=\"step\")\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Cria as vari√°veis das redes online e target e sincroniza inicialmente.\"\"\"\n",
    "        input_spec = tf.TensorSpec(self.observation_space.shape, dtype=tf.float32)\n",
    "        tf_utils.create_variables(self.q_net, input_spec)\n",
    "        tf_utils.create_variables(self.target_q_net, input_spec)\n",
    "        self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Compila a DQN loss junto com a Q-network.\"\"\"\n",
    "        self.update_learner = make_update_fn(\n",
    "            make_q_learning_loss(self.q_net, self.target_q_net, gamma=self.gamma),\n",
    "            self.q_net.trainable_variables,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def step(self, obs, training=True):\n",
    "        \"\"\"Escolhe a a√ß√£o para a observa√ß√£o dada.\"\"\"\n",
    "        obs = tf.convert_to_tensor(obs, dtype=tf.float32)\n",
    "        action = self.policy(obs) if training else tf.argmax(self.q_net(obs), axis=-1)\n",
    "        return action.numpy()\n",
    "\n",
    "    def learn(self, batch):\n",
    "        \"\"\"Recebe um batch de experi√™ncias, atualiza os par√¢metros das redes, e devolve algumas m√©tricas.\"\"\"\n",
    "        # atualiza q_net\n",
    "        loss, grads_and_vars = self.update_learner(batch)\n",
    "\n",
    "        # sincroniza target_q_net\n",
    "        self._step.assign_add(1)\n",
    "        if self._step % self.target_update_freq == 0:\n",
    "            self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "        # m√©tricas de monitoramento\n",
    "        stats = {\n",
    "            \"loss\": loss,\n",
    "            \"q_values_mean\": tf.reduce_mean(self.q_net(batch[\"obs\"])),\n",
    "            \"epsilon\": self.policy._epsilon,\n",
    "            \"vars\": {key: variable for key, (_, variable) in grads_and_vars.items()},\n",
    "            \"grads\": {f\"grad_{key}\": grad for key, (grad, _) in grads_and_vars.items()},\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Salva o estado atual do agente (i.e., o valor dos par√¢metros da rede online) nesse momento.\"\"\"\n",
    "        return self._ckpt_manager.save()\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"Carrega o √∫ltimo checkpoint salvo anteriormente no `save_path`.\"\"\"\n",
    "        if not save_path:\n",
    "            save_path = self._ckpt_manager.latest_checkpoint\n",
    "        return self._ckpt.restore(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-navigation",
   "metadata": {
    "id": "authorized-navigation"
   },
   "source": [
    "## Protocolo de treinamento, avalia√ß√£o e teste\n",
    "\n",
    "Com a classe do `DQN` definida √© hora de treinar o agente e avali√°-lo. Faremos isso seguindo um protocolo de treinamento e avalia√ß√£o definido pelas fun√ß√µes `train` e  `evaluate` abaixo.\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/rl-training.png?raw=true\"/>\n",
    "\n",
    "Tente entender como os hiperpar√¢metros de in√≠cio de treinamento `learning_starts` e frequ√™ncia de atualiza√ß√µes `learn_every` e avalia√ß√£o `evaluation_freq` definem o protocolo.\n",
    "\n",
    "> **Observa√ß√£o**: note que embora o protocolo abaixo seja bastante gen√©rico, tenha em mente que diferentes trabalhos alteram a maneira como os processos de coleta de dados, aprendizado e avalia√ß√£o se intercalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mobile-sixth",
   "metadata": {
    "id": "mobile-sixth"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    test_env,\n",
    "    replay,\n",
    "    logger,\n",
    "    total_timesteps=20_000,\n",
    "    learning_starts=1_000,\n",
    "    learn_every=1,\n",
    "    evaluation_freq=1_000\n",
    "):  \n",
    "    timesteps = 0\n",
    "    episodes = 0\n",
    "    episode_returns = deque(maxlen=100)\n",
    "\n",
    "    best_episode_reward_mean = -np.inf\n",
    "    \n",
    "    with trange(total_timesteps, desc=\"training\") as pbar:\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            obs = env.reset()\n",
    "            episode_return = 0.0\n",
    "\n",
    "            for episode_length in range(1, env.spec.max_episode_steps + 1):\n",
    "\n",
    "                # collect\n",
    "                action = agent.step(np.expand_dims(obs, axis=0), training=True)[0]\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "                timesteps += 1\n",
    "                episode_return += reward\n",
    "\n",
    "                # add experience to replay buffer\n",
    "                terminal = done if episode_length < env.spec.max_episode_steps else False\n",
    "                replay.add(obs, action, reward, terminal, next_obs)\n",
    "\n",
    "                # training\n",
    "                if timesteps >= learning_starts and timesteps % learn_every == 0:\n",
    "                    batch = replay.sample()\n",
    "                    stats = agent.learn(batch)\n",
    "                    stats[\"episode_return_mean\"] = np.mean(episode_returns)\n",
    "                    logger.log(timesteps, stats, label=\"train\") # logging\n",
    "\n",
    "                # evaluation\n",
    "                if timesteps % evaluation_freq == 0:\n",
    "                    stats = evaluate(agent, test_env)\n",
    "                    logger.log(timesteps, stats, label=\"evaluation\") # logging\n",
    "\n",
    "                    # checkpointing\n",
    "                    if stats[\"episode_return_mean\"] > best_episode_reward_mean:\n",
    "                        agent.save()\n",
    "                        best_episode_reward_mean = stats[\"episode_return_mean\"]\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            episodes += 1\n",
    "            episode_returns.append(episode_return)\n",
    "\n",
    "            # logging\n",
    "            stats = {\n",
    "                \"episodes\": episodes,\n",
    "                \"episode_length\": episode_length,\n",
    "                \"episode_return\": episode_return,\n",
    "            }\n",
    "            logger.log(timesteps, stats, label=\"collect\")\n",
    "            logger.flush()\n",
    "\n",
    "            pbar.update(episode_length)\n",
    "            pbar.set_postfix(timesteps=timesteps, episodes=episodes, avg_returns=np.mean(episode_returns) if episode_returns else None)\n",
    "\n",
    "    # final evaluation\n",
    "    stats = evaluate(agent, test_env)\n",
    "    logger.log(timesteps, stats, label=\"evaluation\")\n",
    "    logger.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-tablet",
   "metadata": {
    "id": "similar-tablet"
   },
   "source": [
    "Para avaliarmos o agente, utilizaremos o `eval_env` que foi criado como um ambiente paralelizado (contendo `env.num_envs` rodando de forma ass√≠ncrona em paralelo). \n",
    "\n",
    "> **Observa√ß√£o**: Note no c√≥digo abaixo, como esse tipo de ambiente altera ligeiramente o ciclo de intera√ß√£o agente-ambiente que vimos nas √∫ltimas aulas. Para maiores detalhes, consulte a documenta√ß√£o de `gym.vector.make` e o c√≥digo dos m√≥dulos em [https://github.com/openai/gym/tree/master/gym/vector](https://github.com/openai/gym/tree/master/gym/vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "combined-delight",
   "metadata": {
    "id": "combined-delight"
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env):\n",
    "    total_reward = np.zeros((env.num_envs,))\n",
    "    episode_length = np.zeros((env.num_envs,))\n",
    "\n",
    "    obs = env.reset()\n",
    "    dones = np.array([False] * env.num_envs)\n",
    "\n",
    "    while not np.all(dones):\n",
    "        action = agent.step(obs, training=False)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += (1 - dones) * reward\n",
    "        episode_length += (1 - dones)\n",
    "        dones = np.logical_or(dones, done)\n",
    "\n",
    "    return {\n",
    "        \"episode_return_mean\": np.mean(total_reward),\n",
    "        \"episode_return_min\": np.min(total_reward),\n",
    "        \"episode_return_max\": np.max(total_reward),\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-plain",
   "metadata": {
    "id": "substantial-plain"
   },
   "source": [
    "Execute a c√©lula abaixo para definir um ciclo de intera√ß√£o agente-ambiente para renderizar epis√≥dios do agente ap√≥s o treinamento e ent√£o verificar qualitativamente qu√£o boa foi a pol√≠tica que o agente aprendeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "descending-connectivity",
   "metadata": {
    "id": "descending-connectivity"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "TestEpisode = namedtuple(\"TestEpisode\", [\"number\", \"length\", \"accumulated_reward\"])\n",
    "\n",
    "def test(agent, env, episodes=3, wait=None, render=True, print_results=False):\n",
    "    results = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        episode_len = 0\n",
    "        accumulated_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.step(np.expand_dims(obs, axis=0), training=False)[0]\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            episode_len += 1\n",
    "            accumulated_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            if wait:\n",
    "                time.sleep(wait)\n",
    "\n",
    "        episode_result = TestEpisode(episode+1, episode_len, accumulated_reward)\n",
    "        results.append(episode_result)\n",
    "\n",
    "        if print_results:\n",
    "          print(f\"Episode {episode_result.number}\")\n",
    "          print(f\"Length: {episode_result.length}\")\n",
    "          print(f\"Accumulated reward: {episode_result.accumulated_reward}\")\n",
    "          print()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-reservoir",
   "metadata": {
    "id": "informative-reservoir"
   },
   "source": [
    "## Treinando DQN no CartPole-v1\n",
    "\n",
    "Finalmente, temos todo o c√≥digo necess√°rio para treinarmos o `DQN` no `CartPole-v1`. Antes de iniciarmos o treinamento, execute a c√©lula abaixo para instanciarmos o `tensorboard`, a ferramenta de *logging* e monitoramento do TensorFlow. Consulte a documenta√ß√£o e os tutoriais dispon√≠veis em [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard) para maiores informa√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "casual-alpha",
   "metadata": {
    "id": "casual-alpha"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8fc32916a28435d4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8fc32916a28435d4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --reload_interval 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "competent-arlington",
   "metadata": {
    "id": "competent-arlington"
   },
   "outputs": [],
   "source": [
    "def run(env, eval_env, run_name=\"default\", trials=3, \n",
    "        total_timesteps=20_000, learning_starts=1_000, evaluation_freq=1_000,\n",
    "        gamma=0.99, target_update_freq=1000, learning_rate=1e-3, \n",
    "        policy_start_val=1.0, policy_end_val=0.01, policy_start_step=1_000, policy_end_step=10_000,\n",
    "        replay_buffer_batch_size=64):\n",
    "    for _ in range(trials):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n",
    "        run_id = osp.join(f\"dqn-{env.spec.id}-{run_name}\".lower(), timestamp)\n",
    "\n",
    "        logger = logging.TFLogger(run_id, base_dir=\"logs\")\n",
    "\n",
    "        buffer = replay.ReplayBuffer(env.observation_space, env.action_space, max_size=total_timesteps, batch_size=replay_buffer_batch_size)\n",
    "        buffer.build()\n",
    "\n",
    "        agent = DQN(env.observation_space, env.action_space, checkpoint_dir=f\"ckpt/{run_id}\", \n",
    "                    gamma=gamma, target_update_freq=target_update_freq, learning_rate=learning_rate, \n",
    "                    policy_start_val=policy_start_val, policy_end_val=policy_end_val, policy_start_step=policy_start_step, policy_end_step=policy_end_step)\n",
    "\n",
    "        agent.build()\n",
    "        agent.compile()\n",
    "\n",
    "        train(agent, env, eval_env, buffer, logger, total_timesteps=total_timesteps, learning_starts=learning_starts, evaluation_freq=evaluation_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "finnish-organization",
   "metadata": {
    "id": "finnish-organization"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 15:55:39.643711: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-22 15:55:41.105959: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-22 15:55:41.155209: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a2b76b6f33487ba8cbc2ffb375c14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " run(env, eval_env, run_name=\"default\", trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-tours",
   "metadata": {
    "id": "caroline-tours"
   },
   "source": [
    "Agora √© s√≥ buscar um caf√© esperar o resultado do treinamento. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-multiple",
   "metadata": {
    "id": "meaning-multiple"
   },
   "source": [
    "### Teste do Agente no CartPole-v1\n",
    "\n",
    "Escolha um dos agentes treinados acima para testar e visualizar seu comportamento com o c√≥digo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b2ba650-4efd-49b0-bb22-87fe4aded316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m2021-11-22-15:55\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# lista modelos treinados para a rodada \"default\"\n",
    "!ls ./ckpt/dqn-cartpole-v1-default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ideal-circuit",
   "metadata": {
    "id": "ideal-circuit"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TestEpisode(number=1, length=144, accumulated_reward=144.0),\n",
       " TestEpisode(number=2, length=145, accumulated_reward=145.0),\n",
       " TestEpisode(number=3, length=111, accumulated_reward=111.0),\n",
       " TestEpisode(number=4, length=109, accumulated_reward=109.0),\n",
       " TestEpisode(number=5, length=186, accumulated_reward=186.0),\n",
       " TestEpisode(number=6, length=180, accumulated_reward=180.0),\n",
       " TestEpisode(number=7, length=262, accumulated_reward=262.0),\n",
       " TestEpisode(number=8, length=146, accumulated_reward=146.0),\n",
       " TestEpisode(number=9, length=134, accumulated_reward=134.0),\n",
       " TestEpisode(number=10, length=118, accumulated_reward=118.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = \"ckpt/dqn-cartpole-v1-default/2021-11-22-15:55\" # altere essa linha para escolher qual checkpoint do agente\n",
    "\n",
    "agent = DQN(env.observation_space, env.action_space, checkpoint_dir=checkpoint_dir)\n",
    "agent.build()\n",
    "agent.restore()\n",
    "\n",
    "test(agent, test_env, episodes=10, render=False) #render True s√≥ funciona localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X14vamQ4bECR",
   "metadata": {
    "id": "X14vamQ4bECR"
   },
   "source": [
    "### **üí° Atividade 1**: Treinando com outros par√¢metros\n",
    "\n",
    "Dado os exemplos de treinamento acima, o que acontece quando treinamos o modelo com outros par√¢metros?\n",
    "\n",
    "Fa√ßa ao menos tr√™s treinamentos com diferentes par√¢metros e fa√ßa uma compara√ß√£o dos seus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "xplxVsR2a1K5",
   "metadata": {
    "id": "xplxVsR2a1K5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f9608bbc5744758ea75d65e009c475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name=\"reduced_training\", trials=1, total_timesteps=10_000, learning_starts=1_000, evaluation_freq=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xBdzHfbTXaaI",
   "metadata": {
    "id": "xBdzHfbTXaaI"
   },
   "source": [
    "### **üí° Atividade 2**: Aplicando o DQN para outros modelos\n",
    "\n",
    "Treine o DQN utilizando um outro ambiente do OpenAI Gym, o `MountainCar-v0` (descrito [aqui](https://gym.openai.com/envs/MountainCar-v0/)), com a configura√ß√£o de par√¢metros da escolha de voc√™s.\n",
    "\n",
    "A partir de qual configura√ß√£o de par√¢metros o algoritmo passa a aprender a din√¢mica deste dom√≠nio e passa a ter recompensas maiores de `-200`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AuV6mhNibK-e",
   "metadata": {
    "id": "AuV6mhNibK-e"
   },
   "outputs": [],
   "source": [
    "#env, eval_env, test_env = make_envs(\"MountainCar-v0\")\n",
    "#run(env, eval_env, run_name=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SaZXQQ-OF8Zn",
   "metadata": {
    "id": "SaZXQQ-OF8Zn"
   },
   "source": [
    "### **üí° Atividade 3**: Discuss√£o\n",
    "\n",
    "Aqui iremos discutir os resultados das atividades 1 e 2 e explicar os comportamentos do algoritmo com as varia√ß√µes de hiper-par√¢metros\n",
    "\n",
    "Se voc√™s acharem necess√°rio (e na verdade encorajamos voc√™s), sintam-se a vontade para montar gr√°ficos para embasar a explica√ß√£o de voc√™s (seja por pyplot ou por prints dos gr√°ficos do tensorboard).\n",
    "\n",
    "As perguntas abaixo servem como um guia do que pode ser discutido:\n",
    "\n",
    "- Como a escolha dos par√¢metros interfere no tempo de treinamento do algoritmo?\n",
    "- Qual foi o impacto da altera√ß√£o de um hiperpar√¢metro na performance do mesmo?\n",
    "- Ao utilizar um agente j√° treinado no teste, o seu resultado foi consistente com o treinamento?\n",
    "- Que estat√≠sticas √∫teis podem ser considerados para comparar dois agentes treinados?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NbKFiZ0pFg-t",
   "metadata": {
    "id": "NbKFiZ0pFg-t"
   },
   "source": [
    "-- Fazer a discuss√£o aqui --"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MAC0318 - EP Deep RL ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
